{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0872ab1-f269-46e0-b7e2-0cd6bb81f7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Point the executor to the same Python interpreter the driver is using\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e8e689e-2ef7-4d21-86d8-7d9043ff90d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"HDFS_CSV_Test\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d221f47-c7b4-4399-a2b9-3cc35e371673",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_csv_path = \"hdfs://localhost:9000/weather_data/LAweather.csv\"\n",
    "streaming_dir = \"hdfs://localhost:9000/streaming_weather/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bb6781b-a4bd-457b-b971-d7a3d495cb50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+--------+----------+---------+--------------------+----+---------------+----+---------------+------+--------------+-----+--------------+-----+----------------+----+---------------+-----+-----+----+--------------+----+--------------+----+---------------+-----+------+\n",
      "|    STATION|      DATE|LATITUDE| LONGITUDE|ELEVATION|                NAME|TEMP|TEMP_ATTRIBUTES|DEWP|DEWP_ATTRIBUTES|   SLP|SLP_ATTRIBUTES|  STP|STP_ATTRIBUTES|VISIB|VISIB_ATTRIBUTES|WDSP|WDSP_ATTRIBUTES|MXSPD| GUST| MAX|MAX_ATTRIBUTES| MIN|MIN_ATTRIBUTES|PRCP|PRCP_ATTRIBUTES| SNDP|FRSHTT|\n",
      "+-----------+----------+--------+----------+---------+--------------------+----+---------------+----+---------------+------+--------------+-----+--------------+-----+----------------+----+---------------+-----+-----+----+--------------+----+--------------+----+---------------+-----+------+\n",
      "|74505753130|2025-01-01|34.25917|-118.41333|    305.7|LOS ANGELES WHITE...|62.6|           10.0|28.8|            9.0|9999.9|           0.0|982.9|           7.0| 10.0|            10.0| 1.8|           10.0|  7.0|999.9|69.8|             *|48.2|             *| 0.0|              I|999.9|     0|\n",
      "|74505753130|2025-01-02|34.25917|-118.41333|    305.7|LOS ANGELES WHITE...|68.4|           13.0|27.2|           13.0|9999.9|           0.0|982.6|          12.0| 10.0|            13.0| 6.0|           13.0| 12.0| 14.0|78.8|             *|51.8|             *| 0.0|              I|999.9|     0|\n",
      "|74505753130|2025-01-03|34.25917|-118.41333|    305.7|LOS ANGELES WHITE...|64.0|           12.0|27.6|           12.0|9999.9|           0.0|981.6|          12.0| 10.0|            12.0| 4.5|           12.0| 12.0|999.9|71.6|             *|55.4|             *| 0.0|              I|999.9|     0|\n",
      "|74505753130|2025-01-04|34.25917|-118.41333|    305.7|LOS ANGELES WHITE...|60.4|           12.0|36.4|           12.0|9999.9|           0.0|982.4|          12.0|  9.5|            11.0| 7.3|           12.0| 17.1| 25.1|64.4|             *|53.6|             *| 0.0|              I|999.9|     0|\n",
      "|74505753130|2025-01-05|34.25917|-118.41333|    305.7|LOS ANGELES WHITE...|65.4|            9.0|28.0|            9.0|9999.9|           0.0|983.7|           9.0|  9.8|             9.0| 2.5|            9.0|  5.1|999.9|75.2|             *|50.0|             *| 0.0|              I|999.9|     0|\n",
      "+-----------+----------+--------+----------+---------+--------------------+----+---------------+----+---------------+------+--------------+-----+--------------+-----+----------------+----+---------------+-----+-----+----+--------------+----+--------------+----+---------------+-----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(original_csv_path, header=True, inferSchema=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e87da22-0480-414e-85d2-3b88862f1cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b7b9ce0-cec1-40f3-a2fe-eb37f39ce097",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2eae072a-05a0-4f83-a9d9-f181103d2d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "total_rows = len(pandas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ce01644-5fc9-44e7-bb41-50ea0958fef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_used = 0\n",
    "batch_num = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4452792-7eee-4d5e-9cf0-5e917ae5ba73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 written to HDFS\n",
      "Batch 2 written to HDFS\n",
      "Batch 3 written to HDFS\n",
      "Batch 4 written to HDFS\n",
      "Batch 5 written to HDFS\n",
      "Batch 6 written to HDFS\n",
      "Batch 7 written to HDFS\n",
      "Batch 8 written to HDFS\n",
      "Batch 9 written to HDFS\n",
      "Batch 10 written to HDFS\n",
      "Batch 11 written to HDFS\n",
      "Batch 12 written to HDFS\n",
      "Batch 13 written to HDFS\n",
      "Batch 14 written to HDFS\n",
      "Batch 15 written to HDFS\n",
      "Batch 16 written to HDFS\n",
      "Batch 17 written to HDFS\n",
      "Batch 18 written to HDFS\n",
      "Batch 19 written to HDFS\n",
      "Batch 20 written to HDFS\n",
      "Batch 21 written to HDFS\n",
      "Batch 22 written to HDFS\n",
      "Batch 23 written to HDFS\n",
      "Batch 24 written to HDFS\n",
      "All batches streamed successfully!\n"
     ]
    }
   ],
   "source": [
    "while rows_used < total_rows:\n",
    "    # Select the next batch of rows\n",
    "    batch_df = pandas_df.iloc[rows_used:rows_used + batch_size]\n",
    "    \n",
    "    # Convert batch back to Spark DataFrame\n",
    "    spark_batch_df = spark.createDataFrame(batch_df)\n",
    "    \n",
    "    # Write batch to HDFS\n",
    "    batch_path = os.path.join(streaming_dir, f\"batch_{batch_num}\")\n",
    "    spark_batch_df.write.mode(\"append\").csv(batch_path, header=True)\n",
    "    \n",
    "    print(f\"Batch {batch_num} written to HDFS\")\n",
    "    \n",
    "    # Increment counters\n",
    "    rows_used += batch_size\n",
    "    batch_num += 1\n",
    "    \n",
    "    # Pause to simulate real-time arrival\n",
    "    time.sleep(2)  # 2 seconds between batches\n",
    "\n",
    "print(\"All batches streamed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20983fe3-454e-4640-a56a-4ae0d5fc8a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import input_file_name\n",
    "# from pyspark.sql.functions import lit\n",
    "# from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n",
    "\n",
    "# schema = StructType([\n",
    "#     StructField(\"STATION\", StringType(), True),\n",
    "#     StructField(\"DATE\", StringType(), True),\n",
    "#     StructField(\"LATITUDE\", DoubleType(), True),\n",
    "#     StructField(\"LONGITUDE\", DoubleType(), True),\n",
    "#     StructField(\"ELEVATION\", DoubleType(), True),\n",
    "#     StructField(\"NAME\", StringType(), True),\n",
    "#     StructField(\"TEMP\", DoubleType(), True),\n",
    "#     StructField(\"TEMP_ATTRIBUTES\", StringType(), True),\n",
    "#     StructField(\"DEWP\", DoubleType(), True),\n",
    "#     StructField(\"DEWP_ATTRIBUTES\", StringType(), True),\n",
    "#     StructField(\"SLP\", DoubleType(), True),\n",
    "#     StructField(\"SLP_ATTRIBUTES\", StringType(), True),\n",
    "#     StructField(\"STP\", DoubleType(), True),\n",
    "#     StructField(\"STP_ATTRIBUTES\", StringType(), True),\n",
    "#     StructField(\"VISIB\", DoubleType(), True),\n",
    "#     StructField(\"VISIB_ATTRIBUTES\", StringType(), True),\n",
    "#     StructField(\"WDSP\", DoubleType(), True),\n",
    "#     StructField(\"WDSP_ATTRIBUTES\", StringType(), True),\n",
    "#     StructField(\"MXSPD\", DoubleType(), True),\n",
    "#     StructField(\"GUST\", DoubleType(), True),\n",
    "#     StructField(\"MAX\", DoubleType(), True),\n",
    "#     StructField(\"MAX_ATTRIBUTES\", StringType(), True),\n",
    "#     StructField(\"MIN\", DoubleType(), True),\n",
    "#     StructField(\"MIN_ATTRIBUTES\", StringType(), True),\n",
    "#     StructField(\"PRCP\", DoubleType(), True),\n",
    "#     StructField(\"PRCP_ATTRIBUTES\", StringType(), True),\n",
    "#     StructField(\"SNDP\", DoubleType(), True),\n",
    "#     StructField(\"FRSHTT\", StringType(), True)\n",
    "# ])\n",
    "\n",
    "\n",
    "# from pyspark.sql.functions import lit\n",
    "\n",
    "# df_stream = spark.readStream \\\n",
    "#     .option(\"header\", True) \\\n",
    "#     .schema(schema) \\\n",
    "#     .csv(\"hdfs://localhost:9000/streaming_weather\")\n",
    "\n",
    "# # Add a debug column to see stream is active\n",
    "# df_stream_debug = df_stream.withColumn(\"status\", lit(\"Stream active\"))\n",
    "\n",
    "\n",
    "# # Add a column showing which file Spark read\n",
    "\n",
    "# # Add a constant column to verify streaming is alive\n",
    "# df_test = df_stream.withColumn(\"status\", lit(\"ðŸš€ Stream is working!\"))\n",
    "\n",
    "# query = df_test.writeStream \\\n",
    "#     .outputMode(\"append\") \\\n",
    "#     .format(\"console\") \\\n",
    "#     .option(\"truncate\", False) \\\n",
    "#     .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af640fa-0720-4da3-b8e2-dfba2deb536b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
