{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0872ab1-f269-46e0-b7e2-0cd6bb81f7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Point the executor to the same Python interpreter the driver is using\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e8e689e-2ef7-4d21-86d8-7d9043ff90d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"HDFS_CSV_Test\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d221f47-c7b4-4399-a2b9-3cc35e371673",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_csv_path = \"hdfs://localhost:9000/weather_data/LAweather.csv\"\n",
    "streaming_dir = \"hdfs://localhost:9000/streaming_weather/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6bb6781b-a4bd-457b-b971-d7a3d495cb50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+--------+----------+---------+--------------------+----+---------------+----+---------------+------+--------------+-----+--------------+-----+----------------+----+---------------+-----+-----+----+--------------+----+--------------+----+---------------+-----+------+\n",
      "|    STATION|      DATE|LATITUDE| LONGITUDE|ELEVATION|                NAME|TEMP|TEMP_ATTRIBUTES|DEWP|DEWP_ATTRIBUTES|   SLP|SLP_ATTRIBUTES|  STP|STP_ATTRIBUTES|VISIB|VISIB_ATTRIBUTES|WDSP|WDSP_ATTRIBUTES|MXSPD| GUST| MAX|MAX_ATTRIBUTES| MIN|MIN_ATTRIBUTES|PRCP|PRCP_ATTRIBUTES| SNDP|FRSHTT|\n",
      "+-----------+----------+--------+----------+---------+--------------------+----+---------------+----+---------------+------+--------------+-----+--------------+-----+----------------+----+---------------+-----+-----+----+--------------+----+--------------+----+---------------+-----+------+\n",
      "|74505753130|2025-01-01|34.25917|-118.41333|    305.7|LOS ANGELES WHITE...|62.6|           10.0|28.8|            9.0|9999.9|           0.0|982.9|           7.0| 10.0|            10.0| 1.8|           10.0|  7.0|999.9|69.8|             *|48.2|             *| 0.0|              I|999.9|     0|\n",
      "|74505753130|2025-01-02|34.25917|-118.41333|    305.7|LOS ANGELES WHITE...|68.4|           13.0|27.2|           13.0|9999.9|           0.0|982.6|          12.0| 10.0|            13.0| 6.0|           13.0| 12.0| 14.0|78.8|             *|51.8|             *| 0.0|              I|999.9|     0|\n",
      "|74505753130|2025-01-03|34.25917|-118.41333|    305.7|LOS ANGELES WHITE...|64.0|           12.0|27.6|           12.0|9999.9|           0.0|981.6|          12.0| 10.0|            12.0| 4.5|           12.0| 12.0|999.9|71.6|             *|55.4|             *| 0.0|              I|999.9|     0|\n",
      "|74505753130|2025-01-04|34.25917|-118.41333|    305.7|LOS ANGELES WHITE...|60.4|           12.0|36.4|           12.0|9999.9|           0.0|982.4|          12.0|  9.5|            11.0| 7.3|           12.0| 17.1| 25.1|64.4|             *|53.6|             *| 0.0|              I|999.9|     0|\n",
      "|74505753130|2025-01-05|34.25917|-118.41333|    305.7|LOS ANGELES WHITE...|65.4|            9.0|28.0|            9.0|9999.9|           0.0|983.7|           9.0|  9.8|             9.0| 2.5|            9.0|  5.1|999.9|75.2|             *|50.0|             *| 0.0|              I|999.9|     0|\n",
      "+-----------+----------+--------+----------+---------+--------------------+----+---------------+----+---------------+------+--------------+-----+--------------+-----+----------------+----+---------------+-----+-----+----+--------------+----+--------------+----+---------------+-----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(original_csv_path, header=True, inferSchema=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e87da22-0480-414e-85d2-3b88862f1cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b7b9ce0-cec1-40f3-a2fe-eb37f39ce097",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2eae072a-05a0-4f83-a9d9-f181103d2d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "total_rows = len(pandas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0ce01644-5fc9-44e7-bb41-50ea0958fef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_used = 0\n",
    "batch_num = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c4452792-7eee-4d5e-9cf0-5e917ae5ba73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 written to HDFS\n",
      "Batch 2 written to HDFS\n",
      "Batch 3 written to HDFS\n",
      "Batch 4 written to HDFS\n",
      "Batch 5 written to HDFS\n",
      "Batch 6 written to HDFS\n",
      "Batch 7 written to HDFS\n",
      "Batch 8 written to HDFS\n",
      "Batch 9 written to HDFS\n",
      "Batch 10 written to HDFS\n",
      "Batch 11 written to HDFS\n",
      "Batch 12 written to HDFS\n",
      "Batch 13 written to HDFS\n",
      "Batch 14 written to HDFS\n",
      "Batch 15 written to HDFS\n",
      "Batch 16 written to HDFS\n",
      "Batch 17 written to HDFS\n",
      "Batch 18 written to HDFS\n",
      "Batch 19 written to HDFS\n",
      "Batch 20 written to HDFS\n",
      "Batch 21 written to HDFS\n",
      "Batch 22 written to HDFS\n",
      "Batch 23 written to HDFS\n",
      "Batch 24 written to HDFS\n",
      "All batches streamed successfully!\n"
     ]
    }
   ],
   "source": [
    "while rows_used < total_rows:\n",
    "    # Select the next batch of rows\n",
    "    batch_df = pandas_df.iloc[rows_used:rows_used + batch_size]\n",
    "    \n",
    "    # Convert batch back to Spark DataFrame\n",
    "    spark_batch_df = spark.createDataFrame(batch_df)\n",
    "    \n",
    "    # Write batch to HDFS\n",
    "    batch_path = os.path.join(streaming_dir, f\"batch_{batch_num}\")\n",
    "    spark_batch_df.write.mode(\"overwrite\").csv(batch_path, header=True)\n",
    "    \n",
    "    print(f\"Batch {batch_num} written to HDFS\")\n",
    "    \n",
    "    # Increment counters\n",
    "    rows_used += batch_size\n",
    "    batch_num += 1\n",
    "    \n",
    "    # Pause to simulate real-time arrival\n",
    "    time.sleep(2)  # 2 seconds between batches\n",
    "\n",
    "print(\"All batches streamed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "20983fe3-454e-4640-a56a-4ae0d5fc8a34",
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "Schema must be specified when creating a streaming source DataFrame. If some files already exist in the directory, then depending on the file format you may be able to create a static DataFrame on that directory with 'spark.read.load(directory)' and infer schema from it.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m input_file_name\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m lit\n\u001b[1;32m----> 4\u001b[0m df_stream \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadStream\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mheader\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minferSchema\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhdfs://localhost:9000/streaming_weather\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Add a column showing which file Spark read\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Add a constant column to verify streaming is alive\u001b[39;00m\n\u001b[0;32m     12\u001b[0m df_test \u001b[38;5;241m=\u001b[39m df_stream\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m\"\u001b[39m, lit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mðŸš€ Stream is working!\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\bigdata\\lib\\site-packages\\pyspark\\sql\\streaming\\readwriter.py:715\u001b[0m, in \u001b[0;36mDataStreamReader.csv\u001b[1;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, unescapedQuoteHandling)\u001b[0m\n\u001b[0;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[0;32m    683\u001b[0m     schema\u001b[38;5;241m=\u001b[39mschema,\n\u001b[0;32m    684\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    712\u001b[0m     unescapedQuoteHandling\u001b[38;5;241m=\u001b[39munescapedQuoteHandling,\n\u001b[0;32m    713\u001b[0m )\n\u001b[0;32m    714\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 715\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    716\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    717\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    718\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_STR\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    719\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(path)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m    720\u001b[0m     )\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\bigdata\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\bigdata\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mIllegalArgumentException\u001b[0m: Schema must be specified when creating a streaming source DataFrame. If some files already exist in the directory, then depending on the file format you may be able to create a static DataFrame on that directory with 'spark.read.load(directory)' and infer schema from it."
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import input_file_name\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "df_stream = spark.readStream \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .csv(\"hdfs://localhost:9000/streaming_weather\")\n",
    "\n",
    "# Add a column showing which file Spark read\n",
    "\n",
    "# Add a constant column to verify streaming is alive\n",
    "df_test = df_stream.withColumn(\"status\", lit(\"ðŸš€ Stream is working!\"))\n",
    "\n",
    "query = df_test.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", False) \\\n",
    "    .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf53c892-22ee-46ff-b9fe-afd652d71933",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
