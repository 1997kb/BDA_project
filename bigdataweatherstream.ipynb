{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f54240b4-ab75-4655-8a90-e663abae0268",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import * # Import all types\n",
    "\n",
    "# Point the executor to the same Python interpreter the driver is using\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "from pyspark.sql.functions import input_file_name\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13ab98a1-6030-4e38-b764-22d5fe11ec8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"StreamingWeather\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cd058e-1771-4a1d-92eb-ea37d47cb36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Batch 0: ðŸŒ¡ï¸ WEATHER ANOMALIES DETECTED! ðŸŒ¡ï¸ ---\n",
      "  -> ALERT for Station USC00326025 on 2/9/2003: TMAX=-13.9Â°C, TMIN=-24.4Â°C, Range=10.5Â°C\n",
      "--- Batch 1: No anomalies. Data is clean and within normal parameters. ---\n",
      "--- Batch 2: ðŸŒ¡ï¸ WEATHER ANOMALIES DETECTED! ðŸŒ¡ï¸ ---\n",
      "  -> ALERT for Station USC00509315 on 1/13/2008: TMAX=-24.4Â°C, TMIN=-28.9Â°C, Range=4.5Â°C\n",
      "--- Batch 3: No anomalies. Data is clean and within normal parameters. ---\n",
      "--- Batch 4: No anomalies. Data is clean and within normal parameters. ---\n",
      "--- Batch 5: ðŸŒ¡ï¸ WEATHER ANOMALIES DETECTED! ðŸŒ¡ï¸ ---\n",
      "  -> ALERT for Station USR0000LVER on 6/15/2000: TMAX=19.4Â°C, TMIN=-8.3Â°C, Range=27.7Â°C\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# --- 1. Setup ---\n",
    "spark = SparkSession.builder.appName(\"AdvancedWeatherProcessing\").getOrCreate()\n",
    "\n",
    "# Schema reordered to match the header:\n",
    "# ID|DATE|TMAX|TMIN|EVAP|PRCP|Latitude|Longitude|Elevation|TMAX_actual|TMIN_actual|TRANGE\n",
    "\n",
    "weather_schema = StructType([\n",
    "    StructField(\"ID\", StringType(), True),\n",
    "    StructField(\"DATE\", StringType(), True),\n",
    "    StructField(\"TMAX\", FloatType(), True),\n",
    "    StructField(\"TMIN\", FloatType(), True),\n",
    "    StructField(\"EVAP\", FloatType(), True),\n",
    "    StructField(\"PRCP\", FloatType(), True),\n",
    "    StructField(\"Latitude\", FloatType(), True),\n",
    "    StructField(\"Longitude\", FloatType(), True),\n",
    "    StructField(\"Elevation\", FloatType(), True),\n",
    "    StructField(\"TMAX_actual\", FloatType(), True),\n",
    "    StructField(\"TMIN_actual\", FloatType(), True),\n",
    "    StructField(\"TRANGE\", FloatType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "df_stream = spark.readStream \\\n",
    "    .schema(weather_schema) \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"pathGlobFilter\", \"*.csv\") \\\n",
    "    .csv(\"hdfs://localhost:9000/streaming_weather/*\")\n",
    "\n",
    "# --- 2. Clean, Transform, and Validate the Data ---\n",
    "# Step 2.1: Drop rows with null temperature values\n",
    "# clean_df = df_stream.na.drop(subset=[\"TMAX\", \"TMIN\"])\n",
    "\n",
    "# Step 2.2: Transform temperature to actual degrees Celsius and add a 'range' column, Maximum temperature measured in tenths of degrees Celsius.\n",
    "# transformed_df = clean_df.withColumn(\"TMAX_actual\", col(\"TMAX\") / 10.0) \\\n",
    "#                          .withColumn(\"TMIN_actual\", col(\"TMIN\") / 10.0) \\\n",
    "#                          .withColumn(\"TRANGE\", col(\"TMAX_actual\") - col(\"TMIN_actual\"))\n",
    "\n",
    "# # Step 2.3: Add a data quality filter to ensure TMAX is greater than or equal to TMIN\n",
    "# validated_df = transformed_df.filter(col(\"TMAX_actual\") >= col(\"TMIN_actual\"))\n",
    "\n",
    "\n",
    "# --- 3. Define Anomaly and Alerting Logic ---\n",
    "alerts_df = df_stream.filter(\n",
    "    (col(\"TMAX_actual\") > 38.0) |      # Extreme Heat\n",
    "    (col(\"TMIN_actual\") < -20.0) |     # Extreme Cold\n",
    "    (col(\"TRANGE\") > 25.0)             # Extreme daily temperature swing\n",
    ")\n",
    "\n",
    "# --- 4. Create a Function to Process and Print Alerts ---\n",
    "def process_advanced_alerts(batch_df, epoch_id):\n",
    "    \n",
    "    if batch_df.count() > 0:\n",
    "        \n",
    "        print(f\"--- Batch {epoch_id}: ðŸŒ¡ï¸ WEATHER ANOMALIES DETECTED! ðŸŒ¡ï¸ ---\")\n",
    "        \n",
    "        # Collect the alerts and print the details\n",
    "        alerts_to_send = batch_df.select(\"ID\", \"DATE\", \"TMAX_actual\", \"TMIN_actual\", \"TRANGE\").collect()\n",
    "        for alert in alerts_to_send:\n",
    "            print(f\"  -> ALERT for Station {alert['ID']} on {alert['DATE']}: \" \\\n",
    "                  f\"TMAX={alert['TMAX_actual']:.1f}Â°C, TMIN={alert['TMIN_actual']:.1f}Â°C, \" \\\n",
    "                  f\"Range={alert['TRANGE']:.1f}Â°C\")\n",
    "    else:\n",
    "        print(f\"--- Batch {epoch_id}: No anomalies. Data is clean and within normal parameters. ---\")\n",
    "\n",
    "# --- 5. Start the Streaming Query ---\n",
    "query = alerts_df.writeStream \\\n",
    "    .foreachBatch(process_advanced_alerts) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd09f1e-d5d6-427e-b96b-a3f0fc1673be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
